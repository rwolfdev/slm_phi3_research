# Automated Code and Project Documentation with Phi-3  
**Research Repository â€“ Small Language Model (SLM) Evaluation**

This repository contains all research artifacts, datasets, and results from the study  
**â€œAutomated Code and Project Documentation with Phi-3: Development and Evaluation of a Local AI Assistance System.â€**

The research evaluates Microsoftâ€™s **Phi-3 Small Language Model (SLM)** in its ability to generate function, class, and project-level documentation for C# code.  
The dataset and analysis aim to provide transparency, reproducibility, and comparability for future research in automated software documentation.

---

## ğŸ“ Repository Structure
```
slm_phi3_research/
â”‚
â”œâ”€â”€ diagrams/ # Visualization of evaluation results
â”‚ â”œâ”€â”€ correctness.png
â”‚ â”œâ”€â”€ correllation_cc_correctness.png
â”‚ â”œâ”€â”€ errors.png
â”‚ â”œâ”€â”€ hallucinations.png
â”‚ â”œâ”€â”€ readability.png
â”‚ â””â”€â”€ relevance.png
â”‚
â”œâ”€â”€ results/
â”‚ â”œâ”€â”€ evaluated/ # Consolidated and processed evaluation results
â”‚ â”‚ â”œâ”€â”€ Complete_Resultset.xlsx
â”‚ â”‚ â”œâ”€â”€ correctness.csv
â”‚ â”‚ â”œâ”€â”€ correllation_cc_correctness.csv
â”‚ â”‚ â”œâ”€â”€ errors.csv
â”‚ â”‚ â”œâ”€â”€ hallucinations.csv
â”‚ â”‚ â”œâ”€â”€ readability.csv
â”‚ â”‚ â”œâ”€â”€ relevance.csv
â”‚ â”‚ â””â”€â”€ szenario_overview.csv
â”‚ â”‚
â”‚ â””â”€â”€ raw/ # Raw test and prompt results
â”‚ â”œâ”€â”€ prompts/ # Prompt definitions per documentation type
â”‚ â”‚ â”œâ”€â”€ .class/
â”‚ â”‚ â”œâ”€â”€ .function/
â”‚ â”‚ â””â”€â”€ .project/
â”‚ â”‚
â”‚ â””â”€â”€ szenarios/ # Scenario-based input and generated outputs
â”‚ â”œâ”€â”€ SZ001/ â€¦ SZ00X/
â”‚ â”‚ â”œâ”€â”€ code_input.txt
â”‚ â”‚ â”œâ”€â”€ szenario_info.txt
â”‚ â”‚ â”œâ”€â”€ prompt_XXX.txt_result.txt
â”‚ â”‚ â””â”€â”€ prompt_XXX.txt_result_language.txt
â”‚
â””â”€â”€ README.md # (this file)
```


---

## ğŸ§ª Research Overview

### Objective
To evaluate whether **Phi-3**, as a *locally executable Small Language Model*, can generate  
**accurate, readable, and context-relevant** documentation for software projects without relying on cloud-based LLMs.

### Evaluation Setup
- **Model:** Phi-3-mini (local, CPU-based inference)
- **Language:** C#
- **Documentation types:** Function, Class, Project
- **Prompt variations:** 6 for functions, 4 for classes, 4 for projects  
- **Total test runs:** 132

Each scenario includes:
- Original code input (`code_input.txt`)
- Model outputs with and without language specification
- Associated prompt definition
- Evaluation metrics

### Evaluation Metrics
- **Correctness**
- **Readability**
- **Relevance**
- **Error Rate**
- **Hallucination Frequency**
- **Correlation between Cyclomatic Complexity and Correctness**

---

## ğŸ“Š Results Summary

Key results can be found in `/results/evaluated/`:
- **`Complete_Resultset.xlsx`** â€“ aggregated dataset of all evaluations  
- **`correllation_cc_correctness.png`** â€“ shows the relationship between complexity and correctness  
- **`hallucinations.png`** â€“ illustrates model hallucination frequency by documentation type  

---

## ğŸ” How to Use the Data

You can reproduce or extend the analysis as follows:

1. **Explore scenarios** in `/results/raw/szenarios/`  
   Each folder (e.g., `SZ001`, `SZ004`) contains both prompt input and model output.
2. **Inspect evaluated metrics** via the `.csv` files in `/results/evaluated/`.
3. **Review visualizations** in `/diagrams/` to understand the performance patterns.
4. **Use `szenario_overview.csv`** to match scenario identifiers with documentation types.

---

## ğŸ§  License

This dataset and associated scripts are released under the **MIT License**.  
You are free to use, modify, and share the data with proper attribution.
